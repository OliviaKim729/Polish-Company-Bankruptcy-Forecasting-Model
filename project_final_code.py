# -*- coding: utf-8 -*-
"""Project_Final_code.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XjSa_az2Q66NLsqVEKNPFQXpuxt9MKKc
"""

pip install optuna

import warnings
warnings.filterwarnings("ignore")
warnings.filterwarnings("ignore", category=DeprecationWarning)

import numpy as np
import pandas as pd
from scipy.io import arff
from sklearn.impute import SimpleImputer
from imblearn.over_sampling import SMOTE 
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from imblearn.ensemble import BalancedBaggingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import Perceptron
from sklearn import svm

from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import mean_squared_error
from math import sqrt
from sklearn import metrics
from matplotlib import pyplot as plt
from sklearn.metrics import roc_curve, roc_auc_score

from sklearn.metrics import f1_score
from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import classification_report
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix
from sklearn.metrics import roc_curve
from sklearn.metrics import precision_recall_curve
from sklearn.metrics import classification_report
from sklearn.metrics import matthews_corrcoef

from sklearn import preprocessing
from sklearn.metrics import plot_confusion_matrix
from sklearn.utils import resample
import optuna

"""***1. Data Collection***"""

##Data Collection

from google.colab import files
file = files.upload()
d1 = arff.loadarff('3year.arff')
df1 = pd.DataFrame(d1[0])

"""***2. Data Preprocessing***"""

## Data Exploration, Visualization, Preprocessing
def convert_columns_to_float(df) :
  for i in range(0,63) :
   colname = df.columns[i]
   col = getattr(df,colname)
   df[colname]=col.astype(float)
convert_columns_to_float(df1)

def convert_target_to_binary(df):
  df['class']=df['class'].astype(int)
convert_target_to_binary(df1)


def percentage_missing_values(df):
  df.isnull().sum().sort_values(ascending=False)/len(df)
  df_filtered = df[df.columns[df.isnull().mean() < 0.8]]
percentage_missing_values(df1)
df1.head()

pd.set_option('display.max_rows', None)
pd.set_option('display.max_columns', None)

#Data imputation using mean
df1.shape[0]
df1=df1.fillna(df1.mean())
df1.head()
df1.isnull().sum()

df1.describe()

df2 = df1.drop('class',axis=1)
df_norm=(df2-df2.mean())/df2.std()

print(corr_matrix)

#Normalize the data

corr_matrix = df_norm.corr().abs()
import seaborn as sns

# Select upper triangle of correlation matrix
upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))
ig, ax = plt.subplots(figsize=(15,15))
sns.heatmap(upper,linewidths=0.4, ax=ax)
# Find features with correlation greater than 0.95
to_drop = [column for column in upper.columns if any(upper[column] > 0.9)]
# Drop features 
df3 = df_norm.drop(columns=to_drop, axis=1)
df3.shape[1]
df3.head()
print(to_drop)

df_box = pd.DataFrame(df_norm)
df_box['class']= df1['class']

print(df_box.columns)

# multiple unpivot columns
df_melt = pd.melt(df_box, id_vars =['Attr1'], value_vars =['class'],var_name='columns')

a = sns.factorplot(data = df_melt,
                   x = 'class',
                   y = ['Attr1'],
                   kind = 'box', # type of plot
                   col = columns)

df_melt = df_box.melt(id_vars = 'class',
                  value_vars = ['Attr3', 'Attr6', 'Attr7', 'Attr10', 'Attr11', 'Attr14', 'Attr16', 'Attr17', 'Attr22', 'Attr23', 'Attr25', 'Attr26', 'Attr31', 'Attr35', 'Attr38', 'Attr42', 'Attr44', 'Attr46', 'Attr48', 'Attr49', 'Attr50', 'Attr51', 'Attr52', 'Attr53', 'Attr54', 'Attr62'],
                  var_name = 'columns')

a = sns.factorplot(data = df_melt,
                   x = 'class',
                   y = ['Attr3', 'Attr6', 'Attr7', 'Attr10', 'Attr11', 'Attr14', 'Attr16', 'Attr17', 'Attr22', 'Attr23', 'Attr25', 'Attr26', 'Attr31', 'Attr35', 'Attr38', 'Attr42', 'Attr44', 'Attr46', 'Attr48', 'Attr49', 'Attr50', 'Attr51', 'Attr52', 'Attr53', 'Attr54', 'Attr62'],
                   kind = 'box', # type of plot
                   col = 'columns',
                   col_order = ['Attr3', 'Attr6', 'Attr7', 'Attr10', 'Attr11', 'Attr14', 'Attr16', 'Attr17', 'Attr22', 'Attr23', 'Attr25', 'Attr26', 'Attr31', 'Attr35', 'Attr38', 'Attr42', 'Attr44', 'Attr46', 'Attr48', 'Attr49', 'Attr50', 'Attr51', 'Attr52', 'Attr53', 'Attr54', 'Attr62']).set_titles('{col_name}') # remove 'column = ' part of title
plt.figure(figsize = (5,5))
plt.show()

fig, axes = plt.subplots(ncols=7,nrows=7, figsize=(12, 5), sharey=True)
df_box.query("class in ['0', '1']").boxplot(by='class', return_type='axes', ax=axes)

#Data Exploration
df3['class'] = df1['class']
fig = plt.figure(figsize = (15,20))
ax = fig.gca()
df3.hist(ax = ax)

boxplot = df2.boxplot(column=['Attr1', 'Attr9','Attr18'], by=['class'])

pd.plotting.scatter_matrix(df1, alpha=0.2)

fig = plt.figure(figsize = (15,20))
ax = fig.gca()
df3['class'].hist(ax = ax)

#Check for data imbalance

print(df3.groupby('class').size())
minority_percent = (df3['class'].tolist().count(1) / len(df3['class'].tolist()))*100
print('Minority (label 1) percentage: '+  str(minority_percent) + '%')
print('-'*64)

df3.shape

df3.head()

if 'class' in df2:
  print('a')

# convert the data into dataframe format

y = pd.DataFrame(df3['class'])
X = pd.DataFrame(df3.drop('class', axis=1))

X.shape
y.shape

y.head()

from imblearn.over_sampling import RandomOverSampler
ros = RandomOverSampler(random_state=42)
X_resampled, y_resampled = ros.fit_resample(X, y)
from collections import Counter
print(sorted(Counter(y_resampled).items()))

plt.figure(figsize=(30, 30))
heatmap = sns.heatmap(df3.corr(), vmin=-1, vmax=1)

##Visualization_ correlation matrix
import matplotlib.pyplot as plt
import seaborn as sns
def correlation_plot(df):
  df_corr = df.select_dtypes(include=['int64','float64']) #select only float, integer datatype
  display(df_corr.corr())
  plt.matshow(df_corr.corr())
  plt.show()

correlation_plot(df2)

#Visualization_scatter plot
ax1 = df2.plot.scatter(x='Attr1', y='Attr18', c= 'blue')

X = df2.drop('class',1)
M = X.mean()
S = X.std()
N = (X - M)/S
 #conduct PCA with 3 components
pca = PCA(n_components=5)
pca.fit(N)
PCA_5=pd.DataFrame(pca.components_)
print("pca component score :")
print(PCA_5)
 #Explained variance of each component
explained_variance = pd.DataFrame(pca.explained_variance_) 
print("explained variance :")
print( explained_variance.head())
 #Proportion of explained variance of each component
variance_ratio = pd.DataFrame(pca.explained_variance_ratio_)
print("variance ratio :")
print(variance_ratio.head())
variance_ratio.cumsum()

"""***Model 1. Support Vector Machines***



"""

X = df.drop('class', axis=1)

y = df['class']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)
ros = RandomOverSampler(random_state=42)
X_train, y_train = ros.fit_resample(X_train, y_train)
X_test, y_test = ros.fit_resample(X_test, y_test)

X_train_scaled = preprocessing.scale(X_train)
X_test_scaled = preprocessing.scale(X_test)

clf = svm.SVC()
trained_model=clf.fit(X_train_scaled,y_train)
trained_model.fit(X_train_scaled,y_train)
predictions = trained_model.predict(X_test_scaled)

Train_Accuracy = accuracy_score(y_train, trained_model.predict(X_train_scaled))
Test_Accuracy = accuracy_score(y_test, predictions)
Confusion_Matrix = confusion_matrix(y_test, predictions)

print(Confusion_Matrix)
print(Train_Accuracy)
print(Test_Accuracy)

clf = svm.SVC(probability=True)
clf.fit(X_train_scaled,y_train)
pred = clf.predict(X_test_scaled)
ns_probs = [0 for _ in range(len(y_test))]
# predict probabilities
lr_probs = clf.predict_proba(X_test_scaled)
# keep probabilities for the positive outcome only
lr_probs = lr_probs[:, 1]
# calculate scores
ns_auc = roc_auc_score(y_test, ns_probs)
lr_auc = roc_auc_score(y_test, lr_probs)
# summarize scores
print('No Skill: ROC AUC=%.3f' % (ns_auc))
print('SVM: ROC AUC=%.3f' % (lr_auc))
# calculate roc curves
ns_fpr, ns_tpr, _ = roc_curve(y_test, ns_probs)
lr_fpr, lr_tpr, _ = roc_curve(y_test, lr_probs)
# plot the roc curve for the model
plt.plot(ns_fpr, ns_tpr, linestyle='--', label='No Skill')
plt.plot(lr_fpr, lr_tpr, marker='.', label='SVM')
# axis labels
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
# show the legend
plt.legend()
# show the plot
plt.show()

clf = svm.SVC()
clf.fit(X_train_scaled,y_train)
pred = clf.predict(X_test_scaled)


cm=confusion_matrix(y_test,pred,labels=[1,0]) 
df_cm = pd.DataFrame(cm, index = ['TP','FP'],columns = ['FN','TN'])
print(df_cm)
ax = sns.heatmap(cm, annot=True,fmt='d')
plt.title('Heatmap of Confusion Matrix', fontsize=20)
plt.xlabel('Predicted Value',fontsize=15)
plt.ylabel('Actual Value',fontsize=15)

fpr, tpr, thresholds = metrics.roc_curve(y_test, pred)
roc_auc = metrics.auc(fpr, tpr)
display = metrics.RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc,estimator_name='example estimator')
display.plot()
plt.show()

accuracy_list={}
accuracy_list['svm']= float((cm[0,0]+cm[1,1])/cm.sum())
sensitivity_list={}
sensitivity_list['svm']=float(cm[0,0] / (cm[0,1] + cm[0,0]))
precision_list={}
precision_list['svm']=float((cm[0,0]/cm[0,0]+cm[1,0]))

pred = clf.predict(X_test_scaled)
print(classification_report(y_test, pred))
print("MCC:", matthews_corrcoef(y_test, pred))

clf.get_params()

fpr, tpr, thresholds = metrics.roc_curve(y_test, pred)
roc_auc = metrics.auc(fpr, tpr)
display = metrics.RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc,estimator_name='example estimator')
display.plot()
plt.show()

# objective function to be used by optuna study
def objective(trial):
    
    x_kernel = trial.suggest_categorical('kernel', ['linear', 'poly', 'rbf', 'sigmoid'])
    x_C = trial.suggest_float('C', 0.5, 100)
    x_gamma = trial.suggest_categorical('gamma', [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.25, 0.5, 0.75, 1])
    
    classifier_obj = svm.SVC(C=x_C, kernel=x_kernel, gamma=x_gamma)
    
    classifier_obj.fit(X_train_scaled, y_train)
    
    pred_clf = classifier_obj.predict(X_test_scaled)

    #score = f1_score(y_test, pred_clf)
    score = matthews_corrcoef(y_test, pred)
    return score

study = optuna.create_study(direction="maximize") # maximize the Matthews Correlation Coefficient
study.optimize(objective, n_trials=500)
print(study.best_trial)

"""***Model 2. Logistic Regression Model***

"""

import matplotlib.pyplot as plt

##This is the right ROC curve!!


X_train_scaled = preprocessing.scale(X_train)
X_test_scaled = preprocessing.scale(X_test)
clf = LogisticRegression(C=1, penalty='l1', solver='liblinear')
clf.fit(X_train_scaled,y_train)
pred = clf.predict(X_test_scaled)
ns_probs = [0 for _ in range(len(y_test))]
# predict probabilities
lr_probs = clf.predict_proba(X_test_scaled)
# keep probabilities for the positive outcome only
lr_probs = lr_probs[:, 1]
# calculate scores
ns_auc = roc_auc_score(y_test, ns_probs)
lr_auc = roc_auc_score(y_test, lr_probs)
# summarize scores
print('No Skill: ROC AUC=%.3f' % (ns_auc))
print('Logistic: ROC AUC=%.3f' % (lr_auc))
# calculate roc curves
ns_fpr, ns_tpr, _ = roc_curve(y_test, ns_probs)
lr_fpr, lr_tpr, _ = roc_curve(y_test, lr_probs)
# plot the roc curve for the model
plt.plot(ns_fpr, ns_tpr, linestyle='--', label='No Skill')
plt.plot(lr_fpr, lr_tpr, marker='.', label='Logistic')
# axis labels
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
# show the legend
plt.legend()
# show the plot
plt.show()

X_train_scaled = preprocessing.scale(X_train)
X_test_scaled = preprocessing.scale(X_test)
clf = LogisticRegression(C=1, penalty='l1', solver='liblinear')
clf.fit(X_train_scaled,y_train)
pred = clf.predict(X_test_scaled)

cm=confusion_matrix(y_test,pred,labels=[1,0])
df_cm = pd.DataFrame(cm, index = ['TP','FP'],
                  columns = ['FN','TN'])
print(df_cm)
ax = sns.heatmap(cm, annot=True,fmt='d')
plt.title('Heatmap of Confusion Matrix', fontsize=20)
plt.xlabel('Predicted Value',fontsize=15)
plt.ylabel('Actual Value',fontsize=15)

fpr, tpr, thresholds = metrics.roc_curve(y_test, pred)
roc_auc = metrics.auc(fpr, tpr)
display = metrics.RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc,estimator_name='example estimator')
display.plot()
plt.show()

accuracy_list['Logistic']= float((cm[0,0]+cm[1,1])/cm.sum())
sensitivity_list['Logistic']=float(cm[0,0] / (cm[0,1] + cm[0,0]))
precision_list['Logistic']=float((cm[0,0]/cm[0,0]+cm[1,0]))

fpr, tpr, thresholds = metrics.roc_curve(y_test, pred)
roc_auc = metrics.auc(fpr, tpr)
display = metrics.RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc,estimator_name='example estimator')
display.plot()
plt.show()

false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, pred)
plt.clf()
plt.plot(false_positive_rate, true_positive_rate)
plt.xlabel('FPR')
plt.ylabel('TPR')
plt.title('ROC curve-' +  str(clf))
plt.show()

pred = clf.predict(X_test_scaled)
print(classification_report(y_test, pred))
print("MCC:", matthews_corrcoef(y_test, pred))

clf.get_params()

"""***Model 3. Naive Bayse Classifier***"""

##This is the right ROC curve!!

clf = GaussianNB()
clf.fit(X_train_scaled,y_train)
pred = clf.predict(X_test_scaled)
ns_probs = [0 for _ in range(len(y_test))]
# predict probabilities
lr_probs = clf.predict_proba(X_test_scaled)
# keep probabilities for the positive outcome only
lr_probs = lr_probs[:, 1]
# calculate scores
ns_auc = roc_auc_score(y_test, ns_probs)
lr_auc = roc_auc_score(y_test, lr_probs)
# summarize scores
print('No Skill: ROC AUC=%.3f' % (ns_auc))
print('Logistic: ROC AUC=%.3f' % (lr_auc))
# calculate roc curves
ns_fpr, ns_tpr, _ = roc_curve(y_test, ns_probs)
lr_fpr, lr_tpr, _ = roc_curve(y_test, lr_probs)
# plot the roc curve for the model
plt.plot(ns_fpr, ns_tpr, linestyle='--', label='No Skill')
plt.plot(lr_fpr, lr_tpr, marker='.', label='Naive Bayes')
# axis labels
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
# show the legend
plt.legend()
# show the plot
plt.show()

clf = GaussianNB()
clf.fit(X_train_scaled,y_train)
pred = clf.predict(X_test_scaled)


cm=confusion_matrix(y_test,pred,labels=[1,0])
df_cm = pd.DataFrame(cm, index = ['TP','FP'],
                  columns = ['FN','TN'])
print(df_cm)
ax = sns.heatmap(cm, annot=True,fmt='d')
plt.title('Heatmap of Confusion Matrix', fontsize=20)
plt.xlabel('Predicted Value',fontsize=15)
plt.ylabel('Actual Value',fontsize=15)
fpr, tpr, thresholds = metrics.roc_curve(y_test, pred)
roc_auc = metrics.auc(fpr, tpr)
display = metrics.RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc,estimator_name='example estimator')
display.plot()
plt.show()

accuracy_list['Naive_Bayse']= float((cm[0,0]+cm[1,1])/cm.sum())
sensitivity_list['Naive_Bayse']=float(cm[0,0] / (cm[0,1] + cm[0,0]))
precision_list['Naive_Bayse']=float((cm[0,0]/cm[0,0]+cm[1,0]))

pred = clf.predict(X_test_scaled)
print(classification_report(y_test, pred))
print("MCC:", matthews_corrcoef(y_test, pred))

clf.get_params()

"""***KNN Classifier***"""

##This is the right ROC curve!!
from sklearn.neighbors import KNeighborsClassifier
clf = KNeighborsClassifier(n_neighbors=20)
clf.fit(X_train_scaled,y_train)
pred = clf.predict(X_test_scaled)
ns_probs = [0 for _ in range(len(y_test))]
# predict probabilities
lr_probs = clf.predict_proba(X_test_scaled)
# keep probabilities for the positive outcome only
lr_probs = lr_probs[:, 1]
# calculate scores
ns_auc = roc_auc_score(y_test, ns_probs)
lr_auc = roc_auc_score(y_test, lr_probs)
# summarize scores
print('No Skill: ROC AUC=%.3f' % (ns_auc))
print('Logistic: ROC AUC=%.3f' % (lr_auc))
# calculate roc curves
ns_fpr, ns_tpr, _ = roc_curve(y_test, ns_probs)
lr_fpr, lr_tpr, _ = roc_curve(y_test, lr_probs)
# plot the roc curve for the model
plt.plot(ns_fpr, ns_tpr, linestyle='--', label='No Skill')
plt.plot(lr_fpr, lr_tpr, marker='.', label='KNN')
# axis labels
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
# show the legend
plt.legend()
# show the plot
plt.show()

KNN_auc_score = {}
KNN_auc_score['1']=0.803
KNN_auc_score['2']=0.817
KNN_auc_score['3']=0.822
KNN_auc_score['4']=0.817
KNN_auc_score['5']=0.810
KNN_auc_score['6']=0.803
KNN_auc_score['7']=0.796
KNN_auc_score['8']=0.788
KNN_auc_score['9']=0.782
KNN_auc_score['10']=0.778

k = list(KNN_auc_score.keys())           
auc_score = list(KNN_auc_score.values()) 
plt.plot(k,auc_score)
plt.title('AUC score for KNN',fontsize=15)
plt.xlabel('Number of Neighbors',fontsize=12)
plt.ylabel('AUC score',fontsize=12)
plt.show()

from sklearn.neighbors import KNeighborsClassifier
clf = KNeighborsClassifier(n_neighbors=3)
clf.fit(X_train_scaled, y_train)
pred = clf.predict(X_test_scaled)

cm=confusion_matrix(y_test,pred,labels=[1,0])
df_cm = pd.DataFrame(cm, index = ['TP','FP'],
                  columns = ['FN','TN'])
print(df_cm)
ax = sns.heatmap(cm, annot=True,fmt='d')
plt.title('Heatmap of Confusion Matrix', fontsize=20)
plt.xlabel('Predicted Value',fontsize=15)
plt.ylabel('Actual Value',fontsize=15)
fpr, tpr, thresholds = metrics.roc_curve(y_test, pred)
roc_auc = metrics.auc(fpr, tpr)
display = metrics.RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc)
display.plot()
plt.show()

print(float((cm[0,0]+cm[1,1])/cm.sum()))
accuracy_list['KNN']=float((cm[0,0]+cm[1,1])/cm.sum())
sensitivity_list['KNN']=float(cm[0,0] / (cm[0,1] + cm[0,0]))
precision_list['KNN']=float((cm[0,0]/cm[0,0]+cm[1,0]))

print(classification_report(y_test, pred))
print("MCC:", matthews_corrcoef(y_test, pred))

clf.get_params()

"""***Decision Tree Classifier***"""

##This is the right ROC curve!!
from sklearn import tree
clf = tree.DecisionTreeClassifier(max_depth=2)
clf.fit(X_train_scaled,y_train)
pred = clf.predict(X_test_scaled)
ns_probs = [0 for _ in range(len(y_test))]
# predict probabilities
lr_probs = clf.predict_proba(X_test_scaled)
# keep probabilities for the positive outcome only
lr_probs = lr_probs[:, 1]
# calculate scores
ns_auc = roc_auc_score(y_test, ns_probs)
lr_auc = roc_auc_score(y_test, lr_probs)
# summarize scores
print('No Skill: ROC AUC=%.3f' % (ns_auc))
print('Logistic: ROC AUC=%.3f' % (lr_auc))
# calculate roc curves
ns_fpr, ns_tpr, _ = roc_curve(y_test, ns_probs)
lr_fpr, lr_tpr, _ = roc_curve(y_test, lr_probs)
# plot the roc curve for the model
plt.plot(ns_fpr, ns_tpr, linestyle='--', label='No Skill')
plt.plot(lr_fpr, lr_tpr, marker='.', label='DecisionTree')
# axis labels
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
# show the legend
plt.legend()
# show the plot
plt.show()

D={}
D['1']=0.593
D['2']=0.693
D['3']=0.545
D['4']=0.561
D['5']=0.617
D['6']=0.672
D['7']=0.653
D['8']=0.586
D['9']=0.660
D['10']=0.638
n = list(D.keys())           
auc_score = list(D.values()) 
plt.plot(k,auc_score)
plt.title('AUC score for Decision Tree',fontsize=15)
plt.xlabel('Maximum Depth',fontsize=12)
plt.ylabel('AUC score',fontsize=12)
plt.show()

from sklearn import tree
clf = tree.DecisionTreeClassifier(max_depth=2)
clf = clf.fit(X_train_scaled, y_train)
pred = clf.predict(X_test_scaled)

cm=confusion_matrix(y_test,pred,labels=[1,0])
df_cm = pd.DataFrame(cm, index = ['TP','FP'],
                  columns = ['FN','TN'])
print(df_cm)
ax = sns.heatmap(cm, annot=True,fmt='d')
plt.title('Heatmap of Confusion Matrix', fontsize=20)
plt.xlabel('Predicted Value',fontsize=15)
plt.ylabel('Actual Value',fontsize=15)
fpr, tpr, thresholds = metrics.roc_curve(y_test, pred)
roc_auc = metrics.auc(fpr, tpr)
display = metrics.RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc)
display.plot()
plt.show()

print(float((cm[0,0]+cm[1,1])/cm.sum()))
accuracy_list['DecisionTree']= float((cm[0,0]+cm[1,1])/cm.sum())
sensitivity_list['DecisionTree']=float(cm[0,0] / (cm[0,1] + cm[0,0]))
precision_list['DecisionTree']=float((cm[0,0]/cm[0,0]+cm[1,0]))

print(classification_report(y_test, pred))
print("MCC:", matthews_corrcoef(y_test, pred))

"""***Model Ranking***"""

#Comparison of Accuracy
print(accuracy_list)
names = list(accuracy_list.keys())
values = list(accuracy_list.values())
plt.bar(range(len(accuracy_list)), values, tick_label=names)

plt.show()

#Comparison of Sensitivity
print(sensitivity_list)
names = list(sensitivity_list.keys())
values = list(sensitivity_list.values())
plt.bar(range(len(accuracy_list)), values, tick_label=names, color = 'g')
plt.show()

#Comparison of Precision
print(precision_list)
names = list(precision_list.keys())
values = list(precision_list.values())
plt.bar(range(len(precision_list)), values, tick_label=names, color = 'r')
plt.show()

keys_acc = accuracy_list.keys()
values_acc = accuracy_list. values()
max_acc_model = max(accuracy_list, key=accuracy_list.get)
max_acc_value = max(values_acc)

keys_sen = sensitivity_list.keys()
values_sen = sensitivity_list. values()
max_sen_model = max(sensitivity_list, key=sensitivity_list.get)
max_sen_value = max(values_sen)

keys_prec = precision_list.keys()
values_prec = precision_list. values()
max_prec_model = max(precision_list, key=precision_list.get)
max_prec_value = max(values_prec)

print("best model for accuracy :", max_acc_model,max_acc_value,
      "best model for sensitivity :",max_sen_model,max_sen_value,
      "best model for precision :", max_prec_model,max_prec_value)